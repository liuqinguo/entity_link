{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import random\n",
    "from tqdm import tqdm, trange\n",
    "import torch\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "from pytorch_pretrained_bert.modeling import BertModel\n",
    "from pytorch_pretrained_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE\n",
    "from torch.utils.data import  RandomSampler, DataLoader\n",
    "from model.model_builder_out_model import ModelBuilder\n",
    "from model.img_resnet_2_2 import ResNet34\n",
    "from el_dataset2 import  Bert_Res_Data\n",
    "from torch import optim\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(out, labels):\n",
    "    outputs = np.argmax(out, axis=1)\n",
    "    return np.sum(outputs == labels)\n",
    "class Option:\n",
    "    train_file = './data/train'\n",
    "    val_file = './data/valid'\n",
    "    test_file = './data/test'\n",
    "    vid_pic_path = './data/vid'\n",
    "    cid_pic_path = './data/cid'\n",
    "    \n",
    "    bert_model = 'bert-base-chinese'\n",
    "    res_input_dim = 4096\n",
    "    res_hidden_dim = 512\n",
    "    fc_input_dim = 768+768+2048*2\n",
    "    fc_hidden_dim = 512\n",
    "    input_size = 227\n",
    "    class_num = 2\n",
    "    max_seq_length = 128\n",
    "    train_batch_size = 24\n",
    "    eval_batch_size = 16\n",
    "    num_train_epochs = 10\n",
    "    do_lower_case = False\n",
    "\n",
    "opt = Option()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/06/2020 11:52:17 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese.tar.gz from cache at /root/.pytorch_pretrained_bert/42d4a64dda3243ffeca7ec268d5544122e67d9d06b971608796b483925716512.02ac7d664cff08d793eb00d6aac1d04368a1322435e5fe0a27c70b0b3a85327f\n",
      "02/06/2020 11:52:17 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /root/.pytorch_pretrained_bert/42d4a64dda3243ffeca7ec268d5544122e67d9d06b971608796b483925716512.02ac7d664cff08d793eb00d6aac1d04368a1322435e5fe0a27c70b0b3a85327f to temp dir /tmp/tmpixvc9pf7\n",
      "02/06/2020 11:52:21 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_seq_length\": 128,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ModelBuilder(\n",
       "  (text_model): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 768)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (img_model): ResNet34(\n",
       "    (resnet_layer): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace)\n",
       "      (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (4): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): BasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): BasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (4): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (5): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (7): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): BasicBlock(\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (8): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
       "    )\n",
       "  )\n",
       "  (fc1): Sequential(\n",
       "    (0): Linear(in_features=1536, out_features=512, bias=True)\n",
       "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (fc2): Sequential(\n",
       "    (0): Linear(in_features=2816, out_features=512, bias=True)\n",
       "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (fc3): Sequential(\n",
       "    (0): Linear(in_features=4096, out_features=512, bias=True)\n",
       "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (fc4): Sequential(\n",
       "    (0): Linear(in_features=2816, out_features=512, bias=True)\n",
       "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (fc5): Linear(in_features=512, out_features=2, bias=True)\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=128, bias=True)\n",
       "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=128, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_model = BertModel.from_pretrained(opt.bert_model)\n",
    "img_model = ResNet34()\n",
    "model = ModelBuilder(text_model,img_model,opt)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torchvision/transforms/transforms.py:188: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
      "  \"please use transforms.Resize instead.\")\n",
      "02/06/2020 11:22:04 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt from cache at /root/.pytorch_pretrained_bert/8a0c070123c1f794c42a29c6904beb7c1b8715741e235bee04aca2c7636fc83f.9b42061518a39ca00b8b52059fd2bede8daa613f8a8671500e518a8c29de8c00\n",
      "02/06/2020 11:22:47 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt from cache at /root/.pytorch_pretrained_bert/8a0c070123c1f794c42a29c6904beb7c1b8715741e235bee04aca2c7636fc83f.9b42061518a39ca00b8b52059fd2bede8daa613f8a8671500e518a8c29de8c00\n"
     ]
    }
   ],
   "source": [
    "train_data = Bert_Res_Data(opt,tp=\"train\")\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=opt.train_batch_size)\n",
    "\n",
    "val_data = Bert_Res_Data(opt,tp=\"valid\")\n",
    "val_sampler = RandomSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=opt.eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignored_params = list(map(id,model.fc.parameters()))+list(map(id,model.fc1.parameters()))+list(map(id,model.fc2.parameters()))+list(map(id,model.fc3.parameters()))+list(map(id,model.fc4.parameters()))+list(map(id,model.fc5.parameters()))\n",
    "base_params = filter(lambda p: id(p) not in ignored_params,model.parameters())\n",
    "optimizer = optim.SGD([\n",
    "             {'params': base_params},\n",
    "             {'params': model.fc.parameters(), 'lr': 3e-3},{'params': model.fc1.parameters(), 'lr': 3e-3},\n",
    "             {'params': model.fc2.parameters(), 'lr': 3e-3},{'params': model.fc3.parameters(), 'lr': 3e-3},\n",
    "             {'params': model.fc4.parameters(), 'lr': 3e-3},{'params': model.fc5.parameters(), 'lr': 3e-3},\n",
    " \n",
    "             ], lr=3e-4,momentum = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]/opt/conda/lib/python3.6/site-packages/PIL/TiffImagePlugin.py:771: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now loss : 3.0547576260566713 now acc : 0.684047619047619\n",
      "now loss : 2.8324169659614564 now acc : 0.7480952380952381\n",
      "now loss : 2.7129530024528505 now acc : 0.7745238095238095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/PIL/Image.py:2575: DecompressionBombWarning: Image size (104982930 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  DecompressionBombWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now loss : 2.7155805039405823 now acc : 0.7823809523809524\n",
      "now loss : 2.5858731198310854 now acc : 0.8042857142857143\n",
      "now loss : 2.50547204375267 now acc : 0.8202380952380952\n",
      "now loss : 2.4621807670593263 now acc : 0.825\n",
      "now loss : 2.4566389656066896 now acc : 0.8328571428571429\n",
      "now loss : 2.364799156188965 now acc : 0.8426190476190476\n",
      "now loss : 2.346206728219986 now acc : 0.8473809523809523\n",
      "now loss : 2.250629140138626 now acc : 0.8580952380952381\n",
      "0.8725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  20%|██        | 1/5 [58:49<3:55:19, 3529.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now loss : 1.9183097684383392 now acc : 0.9133333333333333\n",
      "now loss : 1.8802788639068604 now acc : 0.9121428571428571\n",
      "now loss : 1.900725886821747 now acc : 0.9085714285714286\n",
      "now loss : 1.9184537160396575 now acc : 0.9088095238095238\n",
      "now loss : 1.8215476977825165 now acc : 0.9192857142857143\n",
      "now loss : 1.8041923916339875 now acc : 0.9197619047619048\n",
      "now loss : 1.8707234025001527 now acc : 0.9102380952380953\n",
      "now loss : 1.8827209961414337 now acc : 0.9085714285714286\n",
      "now loss : 1.813936071395874 now acc : 0.9233333333333333\n",
      "now loss : 1.8459552597999573 now acc : 0.9142857142857143\n",
      "now loss : 1.7649366223812104 now acc : 0.9226190476190477\n",
      "0.9057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  40%|████      | 2/5 [1:57:51<2:56:40, 3533.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now loss : 1.419615039229393 now acc : 0.97\n",
      "now loss : 1.389083769917488 now acc : 0.9707142857142858\n",
      "now loss : 1.4497833943367004 now acc : 0.9569047619047619\n",
      "now loss : 1.4281743454933167 now acc : 0.9595238095238096\n",
      "now loss : 1.4670206594467163 now acc : 0.96\n",
      "now loss : 1.4632092517614366 now acc : 0.9583333333333334\n",
      "now loss : 1.5004987400770187 now acc : 0.9533333333333334\n",
      "now loss : 1.412421048283577 now acc : 0.960952380952381\n",
      "now loss : 1.401915802359581 now acc : 0.9652380952380952\n",
      "now loss : 1.4168664199113845 now acc : 0.9573809523809523\n",
      "now loss : 1.4108772134780885 now acc : 0.9666666666666667\n",
      "0.9142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  60%|██████    | 3/5 [2:56:55<1:57:52, 3536.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now loss : 1.2545413410663604 now acc : 0.9783333333333334\n",
      "now loss : 1.1965620666742325 now acc : 0.9802380952380952\n",
      "now loss : 1.2085674601793288 now acc : 0.9797619047619047\n",
      "now loss : 1.2199348360300064 now acc : 0.9778571428571429\n",
      "now loss : 1.2622296798229218 now acc : 0.9783333333333334\n",
      "now loss : 1.2703646463155747 now acc : 0.9764285714285714\n",
      "now loss : 1.2340271282196045 now acc : 0.9747619047619047\n",
      "now loss : 1.2564669281244278 now acc : 0.9742857142857143\n",
      "now loss : 1.2764370548725128 now acc : 0.9721428571428572\n",
      "now loss : 1.2545706486701966 now acc : 0.974047619047619\n",
      "now loss : 1.3041720026731491 now acc : 0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  80%|████████  | 4/5 [3:56:06<59:00, 3540.95s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9141\n",
      "now loss : 1.0497168171405793 now acc : 0.9876190476190476\n",
      "now loss : 1.0763529980182647 now acc : 0.9852380952380952\n",
      "now loss : 1.1052417212724686 now acc : 0.9830952380952381\n",
      "now loss : 1.1125826114416122 now acc : 0.9823809523809524\n",
      "now loss : 1.1250599616765975 now acc : 0.9847619047619047\n",
      "now loss : 1.1712414747476578 now acc : 0.9783333333333334\n",
      "now loss : 1.1507826375961303 now acc : 0.9809523809523809\n",
      "now loss : 1.1389342993497849 now acc : 0.9840476190476191\n",
      "now loss : 1.150685847401619 now acc : 0.9792857142857143\n",
      "now loss : 1.1671765476465226 now acc : 0.9797619047619047\n",
      "now loss : 1.1017323327064514 now acc : 0.981904761904762\n",
      "0.9191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 5/5 [4:55:17<00:00, 3543.81s/it]\n"
     ]
    }
   ],
   "source": [
    "out_txt = open(\"./record/our_model1\", \"w+\", encoding=\"utf8\")\n",
    "loss_fct = CrossEntropyLoss()\n",
    "model.train()\n",
    "acc = 0\n",
    "for _ in trange(int(5), desc=\"Epoch\"):\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    eval_accuracy = 0\n",
    "    nb_eval_examples = 0\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = tuple(t.cuda() for t in batch)\n",
    "        vid_input_ids,vid_segment_ids,vid_input_mask,cid_input_ids,cid_segment_ids,cid_input_mask,label_ids,img1,img2 = batch\n",
    "        out1,out2,out3,out4,outputs = model(vid_input_ids,vid_segment_ids,vid_input_mask,cid_input_ids,cid_segment_ids,cid_input_mask,img1,img2)\n",
    "        \n",
    "        loss1 = loss_fct(out1.view(-1,opt.class_num), label_ids.view(-1))\n",
    "        loss2 = loss_fct(out2.view(-1,opt.class_num), label_ids.view(-1))\n",
    "        loss3 = loss_fct(out3.view(-1,opt.class_num), label_ids.view(-1))\n",
    "        loss4 = loss_fct(out4.view(-1,opt.class_num), label_ids.view(-1))\n",
    "        loss5 = loss_fct(outputs.view(-1,opt.class_num), label_ids.view(-1))\n",
    "        loss = loss1 + loss2 + loss3 + loss4 +loss5\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        model.zero_grad()\n",
    "\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += vid_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "        outputs = outputs.detach().cpu().numpy()\n",
    "        label_ids = label_ids.to('cpu').numpy()\n",
    "        tmp_eval_accuracy = accuracy(outputs, label_ids)\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        nb_eval_examples += vid_input_ids.size(0)\n",
    "\n",
    "\n",
    "        if (step + 1) % 100 == 0:\n",
    "            eval_accuracy = eval_accuracy / nb_eval_examples\n",
    "            \n",
    "            out_str = \"now loss : {} now acc : {}\".format(tr_loss/100,eval_accuracy)\n",
    "            out_txt.write(\"%s\\n\" %out_str)\n",
    "            print (\"now loss : {} now acc : {}\".format(tr_loss/100,eval_accuracy))\n",
    "            \n",
    "            eval_accuracy = 0\n",
    "            nb_eval_examples = 0\n",
    "            tr_loss = 0\n",
    "\n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    for step, batch in enumerate(val_dataloader):\n",
    "        batch = tuple(t.cuda() for t in batch)\n",
    "        vid_input_ids,vid_segment_ids,vid_input_mask,cid_input_ids,cid_segment_ids,cid_input_mask,label_ids,img1,img2 = batch\n",
    "        with torch.no_grad():\n",
    "            out1,out2,out3,out4,logits = model(vid_input_ids,vid_segment_ids,vid_input_mask,cid_input_ids,cid_segment_ids,cid_input_mask,img1,img2)\n",
    "            #logits = model(input_ids, segment_ids, input_mask)\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = label_ids.to('cpu').numpy()\n",
    "        tmp_eval_accuracy = accuracy(logits, label_ids)\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "        nb_eval_examples += vid_input_ids.size(0)\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "\n",
    "    eval_accuracy = eval_accuracy / nb_eval_examples\n",
    "    \n",
    "    out_str = \"now valid acc : {}\".format(eval_accuracy)\n",
    "    out_txt.write(\"%s\\n\" %out_str)\n",
    "    print (eval_accuracy)\n",
    "    \n",
    "    if eval_accuracy >acc:\n",
    "        acc = eval_accuracy\n",
    "        torch.save(model.state_dict(),'our_model1.pt')\n",
    "    else:\n",
    "        if _ >= 5:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignored_params = list(map(id,model.fc.parameters()))+list(map(id,model.fc1.parameters()))+list(map(id,model.fc2.parameters()))+list(map(id,model.fc3.parameters()))+list(map(id,model.fc4.parameters()))+list(map(id,model.fc5.parameters()))\n",
    "base_params = filter(lambda p: id(p) not in ignored_params,model.parameters())\n",
    "optimizer = optim.SGD([\n",
    "             {'params': base_params},\n",
    "             {'params': model.fc.parameters(), 'lr': 1e-3},{'params': model.fc1.parameters(), 'lr': 1e-3},\n",
    "             {'params': model.fc2.parameters(), 'lr': 1e-3},{'params': model.fc3.parameters(), 'lr': 1e-3},\n",
    "             {'params': model.fc4.parameters(), 'lr': 1e-3},{'params': model.fc5.parameters(), 'lr': 1e-3},\n",
    "\n",
    "             ], lr=2e-4,momentum = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]/opt/conda/lib/python3.6/site-packages/PIL/Image.py:2575: DecompressionBombWarning: Image size (104982930 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  DecompressionBombWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now loss : 1.0166408514976502 now acc : 0.9871428571428571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/PIL/TiffImagePlugin.py:771: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now loss : 0.9409494560956955 now acc : 0.99\n",
      "now loss : 0.9427064245939255 now acc : 0.9907142857142858\n",
      "now loss : 0.944284838438034 now acc : 0.9897619047619047\n",
      "now loss : 0.9668938797712326 now acc : 0.9888095238095238\n",
      "now loss : 0.9356838083267212 now acc : 0.9907142857142858\n",
      "now loss : 0.9869342756271362 now acc : 0.9888095238095238\n",
      "now loss : 0.9637721621990204 now acc : 0.9871428571428571\n",
      "now loss : 0.9672341680526734 now acc : 0.9895238095238095\n",
      "now loss : 0.988777517080307 now acc : 0.9885714285714285\n",
      "now loss : 0.9618419098854065 now acc : 0.9897619047619047\n",
      "0.9212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  10%|█         | 1/10 [59:08<8:52:17, 3548.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now loss : 0.8718844294548035 now acc : 0.9923809523809524\n",
      "now loss : 0.8647763010859489 now acc : 0.9942857142857143\n",
      "now loss : 0.8683710062503814 now acc : 0.9921428571428571\n",
      "now loss : 0.9039366239309311 now acc : 0.9916666666666667\n",
      "now loss : 0.913753269314766 now acc : 0.9907142857142858\n",
      "now loss : 0.9080692803859711 now acc : 0.991904761904762\n",
      "now loss : 0.8908811753988266 now acc : 0.9909523809523809\n",
      "now loss : 0.9109798747301102 now acc : 0.991904761904762\n",
      "now loss : 0.9027325963973999 now acc : 0.9921428571428571\n",
      "now loss : 0.9094808495044708 now acc : 0.9916666666666667\n",
      "now loss : 0.9155768221616745 now acc : 0.9907142857142858\n",
      "0.9225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  20%|██        | 2/10 [1:58:19<7:53:13, 3549.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now loss : 0.8489521640539169 now acc : 0.9935714285714285\n",
      "now loss : 0.8411858195066452 now acc : 0.9938095238095238\n",
      "now loss : 0.8489653521776199 now acc : 0.9947619047619047\n",
      "now loss : 0.8461575162410736 now acc : 0.9942857142857143\n",
      "now loss : 0.8344067996740341 now acc : 0.9938095238095238\n",
      "now loss : 0.8281187915802002 now acc : 0.9952380952380953\n",
      "now loss : 0.8655936473608017 now acc : 0.9935714285714285\n",
      "now loss : 0.8881096476316452 now acc : 0.9930952380952381\n",
      "now loss : 0.8686260879039764 now acc : 0.9914285714285714\n",
      "now loss : 0.8394322943687439 now acc : 0.9926190476190476\n",
      "now loss : 0.862798449397087 now acc : 0.995\n",
      "0.9213\n"
     ]
    }
   ],
   "source": [
    "out_txt = open(\"./record/our_model1\", \"a+\", encoding=\"utf8\")\n",
    "loss_fct = CrossEntropyLoss()\n",
    "model.train()\n",
    "acc = 0\n",
    "for _ in trange(int(opt.num_train_epochs), desc=\"Epoch\"):\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    eval_accuracy = 0\n",
    "    nb_eval_examples = 0\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = tuple(t.cuda() for t in batch)\n",
    "        vid_input_ids,vid_segment_ids,vid_input_mask,cid_input_ids,cid_segment_ids,cid_input_mask,label_ids,img1,img2 = batch\n",
    "        out1,out2,out3,out4,outputs = model(vid_input_ids,vid_segment_ids,vid_input_mask,cid_input_ids,cid_segment_ids,cid_input_mask,img1,img2)\n",
    "        \n",
    "        loss1 = loss_fct(out1.view(-1,opt.class_num), label_ids.view(-1))\n",
    "        loss2 = loss_fct(out2.view(-1,opt.class_num), label_ids.view(-1))\n",
    "        loss3 = loss_fct(out3.view(-1,opt.class_num), label_ids.view(-1))\n",
    "        loss4 = loss_fct(out4.view(-1,opt.class_num), label_ids.view(-1))\n",
    "        loss5 = loss_fct(outputs.view(-1,opt.class_num), label_ids.view(-1))\n",
    "        loss = loss1 + loss2 + loss3 + loss4 +loss5\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        model.zero_grad()\n",
    "\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += vid_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "        outputs = outputs.detach().cpu().numpy()\n",
    "        label_ids = label_ids.to('cpu').numpy()\n",
    "        tmp_eval_accuracy = accuracy(outputs, label_ids)\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        nb_eval_examples += vid_input_ids.size(0)\n",
    "\n",
    "\n",
    "        if (step + 1) % 100 == 0:\n",
    "            eval_accuracy = eval_accuracy / nb_eval_examples\n",
    "            \n",
    "            out_str = \"now loss : {} now acc : {}\".format(tr_loss/100,eval_accuracy)\n",
    "            out_txt.write(\"%s\\n\" %out_str)\n",
    "            print (\"now loss : {} now acc : {}\".format(tr_loss/100,eval_accuracy))\n",
    "            \n",
    "            eval_accuracy = 0\n",
    "            nb_eval_examples = 0\n",
    "            tr_loss = 0\n",
    "\n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    for step, batch in enumerate(val_dataloader):\n",
    "        batch = tuple(t.cuda() for t in batch)\n",
    "        vid_input_ids,vid_segment_ids,vid_input_mask,cid_input_ids,cid_segment_ids,cid_input_mask,label_ids,img1,img2 = batch\n",
    "        with torch.no_grad():\n",
    "            out1,out2,out3,out4,logits = model(vid_input_ids,vid_segment_ids,vid_input_mask,cid_input_ids,cid_segment_ids,cid_input_mask,img1,img2)\n",
    "            #logits = model(input_ids, segment_ids, input_mask)\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = label_ids.to('cpu').numpy()\n",
    "        tmp_eval_accuracy = accuracy(logits, label_ids)\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "        nb_eval_examples += vid_input_ids.size(0)\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "\n",
    "    eval_accuracy = eval_accuracy / nb_eval_examples\n",
    "    \n",
    "    out_str = \"now valid acc : {}\".format(eval_accuracy)\n",
    "    out_txt.write(\"%s\\n\" %out_str)\n",
    "    print (eval_accuracy)\n",
    "    \n",
    "    if eval_accuracy >acc:\n",
    "        acc = eval_accuracy\n",
    "        torch.save(model.state_dict(),'our_model1.pt')\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('./our_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed= 1\n",
    "torch.manual_seed(seed)            # 为CPU设置随机种子\n",
    "torch.cuda.manual_seed(seed)       # 为当前GPU设置随机种子\n",
    "torch.cuda.manual_seed_all(seed) \n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torchvision/transforms/transforms.py:188: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
      "  \"please use transforms.Resize instead.\")\n",
      "02/06/2020 11:52:31 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt from cache at /root/.pytorch_pretrained_bert/8a0c070123c1f794c42a29c6904beb7c1b8715741e235bee04aca2c7636fc83f.9b42061518a39ca00b8b52059fd2bede8daa613f8a8671500e518a8c29de8c00\n"
     ]
    }
   ],
   "source": [
    "test_data = Bert_Res_Data(opt,tp=\"test\")\n",
    "test_sampler = RandomSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data,num_workers = 0,shuffle=False,sampler=None,batch_sampler=None,drop_last = False,  batch_size=opt.train_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/PIL/TiffImagePlugin.py:771: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9281\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "model.eval()\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "y_test = np.array([])\n",
    "y_pred = np.array([])\n",
    "for step, batch in enumerate(test_dataloader):\n",
    "    vid_guids,vid_input_ids,vid_segment_ids,vid_input_mask,cid_guids,cid_input_ids,cid_segment_ids,cid_input_mask,label_ids,img1,img2 = batch\n",
    "    batch = [vid_input_ids,vid_segment_ids,vid_input_mask,cid_input_ids,cid_segment_ids,cid_input_mask,label_ids,img1,img2]\n",
    "    batch = tuple(t.cuda() for t in batch)\n",
    "    vid_input_ids,vid_segment_ids,vid_input_mask,cid_input_ids,cid_segment_ids,cid_input_mask,label_ids,img1,img2 = batch\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out1,out2,out3,out4,logits = model(vid_input_ids,vid_segment_ids,vid_input_mask,cid_input_ids,cid_segment_ids,cid_input_mask,img1,img2)\n",
    "        #logits = model(input_ids, segment_ids, input_mask)\n",
    "    y_pred = np.concatenate((y_pred, logits.max(1)[1].cpu().numpy()))\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = label_ids.to('cpu').numpy()\n",
    "    y_test = np.concatenate((y_test, label_ids))\n",
    "    tmp_eval_accuracy = accuracy(logits, label_ids)\n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "    nb_eval_examples += vid_input_ids.size(0)\n",
    "    nb_eval_steps += 1\n",
    "\n",
    "\n",
    "eval_accuracy = eval_accuracy / nb_eval_examples\n",
    "print (eval_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9379    0.9442    0.9410      6074\n",
      "           1     0.9127    0.9032    0.9080      3926\n",
      "\n",
      "    accuracy                         0.9281     10000\n",
      "   macro avg     0.9253    0.9237    0.9245     10000\n",
      "weighted avg     0.9280    0.9281    0.9280     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, y_pred, digits=4, target_names=['0','1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/PIL/TiffImagePlugin.py:771: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9281\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "out = open(\"./result_our_model\", \"w+\", encoding=\"utf8\")\n",
    "for step, batch in enumerate(test_dataloader):\n",
    "    vid_guids,vid_input_ids,vid_segment_ids,vid_input_mask,cid_guids,cid_input_ids,cid_segment_ids,cid_input_mask,label_ids,img1,img2 = batch\n",
    "    batch = [vid_input_ids,vid_segment_ids,vid_input_mask,cid_input_ids,cid_segment_ids,cid_input_mask,label_ids,img1,img2]\n",
    "    batch = tuple(t.cuda() for t in batch)\n",
    "    vid_input_ids,vid_segment_ids,vid_input_mask,cid_input_ids,cid_segment_ids,cid_input_mask,label_ids,img1,img2 = batch\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out1,out2,out3,out4,logits = model(vid_input_ids,vid_segment_ids,vid_input_mask,cid_input_ids,cid_segment_ids,cid_input_mask,img1,img2)\n",
    "        #logits = model(input_ids, segment_ids, input_mask)\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = label_ids.to('cpu').numpy()\n",
    "    \n",
    "    outputs = np.argmax(logits, axis=1)\n",
    "    #outputs = (outputs == label_ids)\n",
    "   # print(vid_guids)\n",
    "   # print(cid_guids)\n",
    "   # print(outputs)\n",
    "    len_num = len(outputs)\n",
    "    for num in range(len_num):\n",
    "        line = [vid_guids[num],cid_guids[num],str(outputs[num])]\n",
    "        out_str = '\\t'.join(line)\n",
    "        out.write(\"%s\\n\" %out_str)\n",
    "    \n",
    "    tmp_eval_accuracy = accuracy(logits, label_ids)\n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "    nb_eval_examples += vid_input_ids.size(0)\n",
    "    nb_eval_steps += 1\n",
    "\n",
    "\n",
    "eval_accuracy = eval_accuracy / nb_eval_examples\n",
    "print (eval_accuracy)\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "471.0\n"
     ]
    }
   ],
   "source": [
    "print(nb_eval_examples*eval_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9055793991416309\n"
     ]
    }
   ],
   "source": [
    "result_dict = {}\n",
    "with open(\"./result\", \"r\", encoding=\"utf8\") as f:\n",
    "    lines = f.readlines()\n",
    "    for i,line in enumerate(lines):\n",
    "        line = line.strip().split('\\t')\n",
    "        result_dict[line[0]] = line[1]\n",
    "#print(result_dict)   \n",
    "total_num = 0\n",
    "true_num = 0\n",
    "with open(\"./data/testl\", \"r\", encoding=\"utf8\") as f:\n",
    "    lines = f.readlines()\n",
    "    for i,line in enumerate(lines):\n",
    "        total_num += 1\n",
    "        line = line.strip().split('\\t')\n",
    "        if  result_dict[line[1]+line[3]] == 'True':\n",
    "            true_num += 1\n",
    "print(true_num/total_num)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9098712446351931\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "for step, batch in enumerate(test_dataloader):\n",
    "    batch = tuple(t.cuda() for t in batch)\n",
    "    vid_input_ids,vid_segment_ids,vid_input_mask,cid_input_ids,cid_segment_ids,cid_input_mask,label_ids,img1,img2 = batch\n",
    "    with torch.no_grad():\n",
    "        out1,out2,out3,out4,logits = model(vid_input_ids,vid_segment_ids,vid_input_mask,cid_input_ids,cid_segment_ids,cid_input_mask,img1,img2)\n",
    "        #logits = model(input_ids, segment_ids, input_mask)\n",
    "    \n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = label_ids.to('cpu').numpy()\n",
    "    tmp_eval_accuracy = accuracy(logits, label_ids)\n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "    nb_eval_examples += vid_input_ids.size(0)\n",
    "    nb_eval_steps += 1\n",
    "\n",
    "\n",
    "eval_accuracy = eval_accuracy / nb_eval_examples\n",
    "print (eval_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
